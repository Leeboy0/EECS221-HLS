"""
generate_weights.py
-------------------
Trains a 784→256→128→10 MLP on MNIST, quantizes weights to int8,
and exports mnist.h for use with Vitis HLS MLP project.

Requirements:
    pip install torch torchvision numpy

Usage:
    python generate_weights.py

Output:
    mnist.h  — drop this into the Vitis HLS project folder
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import time

# ── Architecture constants (must match MLP.h) ──────────────────────────────
INPUT_SIZE  = 784
LAYER1_SIZE = 256
LAYER2_SIZE = 128
OUTPUT_SIZE = 10

# ── Model definition ───────────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(INPUT_SIZE,  LAYER1_SIZE)
        self.fc2 = nn.Linear(LAYER1_SIZE, LAYER2_SIZE)
        self.fc3 = nn.Linear(LAYER2_SIZE, OUTPUT_SIZE)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(-1, INPUT_SIZE)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)

# ── Training ───────────────────────────────────────────────────────────────
def train():
    transform = transforms.Compose([transforms.ToTensor()])
    train_data = datasets.MNIST('./data', train=True,  download=True, transform=transform)
    test_data  = datasets.MNIST('./data', train=False, download=True, transform=transform)
    train_loader = DataLoader(train_data, batch_size=256, shuffle=True)
    test_loader  = DataLoader(test_data,  batch_size=1000)

    model = MLP()
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    print("Training MLP on MNIST...")
    for epoch in range(10):
        model.train()
        for images, labels in train_loader:
            optimizer.zero_grad()
            loss = criterion(model(images), labels)
            loss.backward()
            optimizer.step()

        # Validation accuracy
        model.eval()
        correct = total = 0
        with torch.no_grad():
            for images, labels in test_loader:
                preds = model(images).argmax(dim=1)
                correct += (preds == labels).sum().item()
                total   += labels.size(0)
        print(f"  Epoch {epoch+1}/10 — Test accuracy: {correct/total:.4f}")

    return model

# ── Quantization helpers ───────────────────────────────────────────────────
def quantize(arr: np.ndarray, n_bits: int = 8):
    """
    Symmetric per-tensor linear quantization to int8.
    Returns (quantized_int8_array, scale_factor)
    where: float_value ≈ quantized * (1 / scale)
    """
    max_val = np.max(np.abs(arr))
    max_int = 2 ** (n_bits - 1) - 1        # 127 for int8
    scale   = max_int / max_val             # multiply float → int
    q       = np.round(arr * scale).astype(np.int8)
    q       = np.clip(q, -max_int, max_int)
    return q, scale

# ── C array formatter ──────────────────────────────────────────────────────
def fmt_array(name: str, dtype: str, data: np.ndarray, cols: int = 16) -> str:
    flat   = data.flatten()
    size   = len(flat)
    values = ', '.join(
        ', '.join(str(int(v)) for v in flat[i:i+cols])
        for i in range(0, size, cols)
    )
    # Insert newlines every cols values for readability
    rows = []
    for i in range(0, size, cols):
        rows.append('    ' + ', '.join(str(int(v)) for v in flat[i:i+cols]))
    body = ',\n'.join(rows)
    return f"const {dtype} {name}[{size}] = {{\n{body}\n}};\n"

# ── Header generation ──────────────────────────────────────────────────────
def generate_header(model: MLP, path: str = "mnist.h"):
    w1 = model.fc1.weight.detach().numpy()   # (256, 784)
    b1 = model.fc1.bias.detach().numpy()     # (256,)
    w2 = model.fc2.weight.detach().numpy()   # (128, 256)
    b2 = model.fc2.bias.detach().numpy()     # (128,)
    w3 = model.fc3.weight.detach().numpy()   # (10,  128)
    b3 = model.fc3.bias.detach().numpy()     # (10,)

    qw1, sw1 = quantize(w1)
    qb1, sb1 = quantize(b1)
    qw2, sw2 = quantize(w2)
    qb2, sb2 = quantize(b2)
    qw3, sw3 = quantize(w3)
    qb3, sb3 = quantize(b3)

    print(f"\nQuantization scale factors:")
    print(f"  weights_1: {sw1:.1f}   (inv = 1/{sw1:.1f})")
    print(f"  bias_1:    {sb1:.1f}   (inv = 1/{sb1:.1f})")
    print(f"  weights_2: {sw2:.1f}   (inv = 1/{sw2:.1f})")
    print(f"  bias_2:    {sb2:.1f}   (inv = 1/{sb2:.1f})")
    print(f"  weights_3: {sw3:.1f}   (inv = 1/{sw3:.1f})")
    print(f"  bias_3:    {sb3:.1f}   (inv = 1/{sb3:.1f})")
    print(f"\n⚠  Update MLP.cpp inv_scale constants with these values!\n")

    lines = [
        "// mnist.h — Auto-generated by generate_weights.py",
        "// MLP weights quantized to int8 for Vitis HLS",
        "// Architecture: 784 → 256 → 128 → 10",
        "//",
        f"// Scale factors (update MLP.cpp inv_scale_ constants to match):",
        f"//   inv_scale_w0 = 1.0 / {sw1:.1f}",
        f"//   inv_scale_b0 = 1.0 / {sb1:.1f}",
        f"//   inv_scale_w1 = 1.0 / {sw2:.1f}",
        f"//   inv_scale_b1 = 1.0 / {sb2:.1f}",
        f"//   inv_scale_w2 = 1.0 / {sw3:.1f}",
        f"//   inv_scale_b2 = 1.0 / {sb3:.1f}",
        "",
        "#ifndef MNIST_H",
        "#define MNIST_H",
        "",
        '#include "ap_int.h"',
        "",
        "typedef ap_int<8> weight_t;",
        "",
        fmt_array("weights_1", "weight_t", qw1),
        fmt_array("bias_1",    "weight_t", qb1),
        fmt_array("weights_2", "weight_t", qw2),
        fmt_array("bias_2",    "weight_t", qb2),
        fmt_array("weights_3", "weight_t", qw3),
        fmt_array("bias_3",    "weight_t", qb3),
        "#endif // MNIST_H",
    ]

    with open(path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    print(f"✅  Generated {path}")
    print(f"    weights_1: {qw1.size:,} values")
    print(f"    weights_2: {qw2.size:,} values")
    print(f"    weights_3: {qw3.size:,} values")

    

if __name__ == "__main__":
    model = train()
    generate_header(model)